{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22115c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from models_cmae import *\n",
    "import models_cmae\n",
    "import models_mae\n",
    "import torch.nn.functional as F\n",
    "from util.datasets import build_dataset\n",
    "import matplotlib\n",
    "from einops import rearrange\n",
    "from matplotlib import pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import os\n",
    "\n",
    "\n",
    "# DEVICE = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48aeb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class args:\n",
    "    data_path: str = '/home/assafsho/data/ILSVRC2012'\n",
    "    input_size: int = 224\n",
    "    color_jitter: bool = False\n",
    "    aa: bool = False\n",
    "    reprob: bool = False\n",
    "    remode: bool = False\n",
    "    recount: bool = False\n",
    "    batch_size: int = 1024\n",
    "    num_workers: int = 0\n",
    "    pin_mem: bool = False\n",
    "    num_groups: int = 4\n",
    "    group_sz: int = 49\n",
    "    mask_ratio: float = 0.\n",
    "\n",
    "\n",
    "# load my model\n",
    "chkpt_dir = '/home/assafsho/mae/only_batchwise/checkpoint-799.pth'\n",
    "model = getattr(models_cmae, 'mae_vit_base_patch16')().to(DEVICE)\n",
    "checkpoint = torch.load(chkpt_dir, map_location='cuda')\n",
    "msg = model.load_state_dict(checkpoint['model'], strict=False)\n",
    "print(msg)\n",
    "\n",
    "\n",
    "# load original mae model\n",
    "# !wget -nc https://dl.fbaipublicfiles.com/mae/visualize/mae_visualize_vit_large.pth\n",
    "!wget -nc https://dl.fbaipublicfiles.com/mae/pretrain/mae_pretrain_vit_base.pth\n",
    "# chkpt_dir = 'mae_visualize_vit_large.pth'\n",
    "chkpt_dir = 'mae_pretrain_vit_base.pth'\n",
    "# model_orig = getattr(models_mae, 'mae_vit_large_patch16')().cuda()\n",
    "model_orig = getattr(models_mae, 'mae_vit_base_patch16')().cuda()\n",
    "checkpoint = torch.load(chkpt_dir, map_location='cuda')\n",
    "msg = model_orig.load_state_dict(checkpoint['model'], strict=False)\n",
    "print(msg)\n",
    "\n",
    "\n",
    "def imshow(x, mask=None, norm=True, orig_sz=True):\n",
    "    imagenet_mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "    imagenet_std = torch.tensor([0.229, 0.224, 0.225])\n",
    "    \n",
    "    if len(x.shape) > 3:\n",
    "        x = x.squeeze(0)\n",
    "    if torch.is_tensor(x):\n",
    "        x = x.detach().cpu()\n",
    "    if torch.is_tensor(mask):\n",
    "        mask = mask.detach().cpu()\n",
    "    if x.shape[0] <= 3:\n",
    "        x = torch.einsum('chw->hwc', x)\n",
    "    if mask is not None and mask.shape[0] <= 3:\n",
    "        mask = torch.einsum('chw->hwc', mask)\n",
    "    if norm:\n",
    "        x = x * imagenet_std + imagenet_mean\n",
    "    if orig_sz:\n",
    "        height, width, depth = x.shape\n",
    "        dpi = matplotlib.rcParams['figure.dpi']\n",
    "        figsize = width / float(dpi), height / float(dpi)\n",
    "        plt.figure(figsize=figsize)\n",
    "    if mask is not None:\n",
    "        x = x * mask\n",
    "    plt.imshow(x.clip(0, 1), vmin=0.)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2329b566",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transform_val = transforms.Compose([\n",
    "            transforms.Resize(256, interpolation=3),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "# dataset_val = build_dataset(is_train=False, args=args)\n",
    "dataset_val = datasets.ImageFolder(os.path.join(args.data_path, 'val'), transform=transform_val)\n",
    "\n",
    "\n",
    "sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "    dataset_val, #sampler=sampler_val,\n",
    "    batch_size=args.batch_size,\n",
    "    num_workers=args.num_workers,\n",
    "    pin_memory=args.pin_mem,\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fca6803",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def nn_evaluate(data_loader, model, model_orig, device, num_queries, num_neighbors):\n",
    "    # assumes use_cls_token=True and data_loader(shuffle=False)\n",
    "\n",
    "    # switch to evaluation mode\n",
    "    model.eval()\n",
    "    full_sim_matrix = torch.empty(num_queries, 0, device=torch.device('cpu'))\n",
    "\n",
    "    query_inds = torch.randperm(50000)[:num_queries]\n",
    "    query_ims = torch.stack([data_loader.dataset[ind][0] for ind in query_inds]).to(device)\n",
    "\n",
    "    with torch.cuda.amp.autocast():\n",
    "        q_reps_full = model(query_ims, num_groups=args.num_groups, group_sz=args.group_sz, encode_only=True, group_duplicates=False)\n",
    "        q_reps_cls = q_reps_full[:, :args.num_groups, :].mean(1)  # [Q, E]\n",
    "        queries = F.normalize(q_reps_cls, dim=-1)\n",
    "            \n",
    "        if model_orig is not None:\n",
    "            model_orig.eval()\n",
    "            q_reps_full_orig, _, _ = model_orig.forward_encoder(query_ims, mask_ratio=args.mask_ratio)\n",
    "            q_reps_cls_orig = q_reps_full_orig[:, 0, :]  # [Q, E]\n",
    "            queries_orig = F.normalize(q_reps_cls_orig, dim=-1)\n",
    "    \n",
    "            full_sim_matrix_orig = torch.empty(num_queries, 0, device=torch.device('cpu'))\n",
    "\n",
    "    \n",
    "    ind = 0\n",
    "    for samples, _ in data_loader:\n",
    "        # get a batch\n",
    "        samples = samples.to(device, non_blocking=True)\n",
    "\n",
    "        # calcualte representations\n",
    "        with torch.cuda.amp.autocast():\n",
    "            representations_full = model(samples, num_groups=args.num_groups, group_sz=args.group_sz, encode_only=True, group_duplicates=False)\n",
    "            rep_cls = representations_full[:, :args.num_groups, :].mean(1)  # [B, E]\n",
    "            reps = F.normalize(rep_cls, dim=-1)\n",
    "            \n",
    "        cur_sim_matrix = torch.einsum('QE,BE->QB', queries, reps)\n",
    "        full_sim_matrix = torch.cat((full_sim_matrix, cur_sim_matrix.to(full_sim_matrix.device)), 1)\n",
    "\n",
    "        if model_orig is not None:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                representations_full_orig, _, _ = model_orig.forward_encoder(samples, mask_ratio=args.mask_ratio)\n",
    "                rep_cls_orig = representations_full_orig[:, 0, :]  # [B, E]\n",
    "                reps_orig = F.normalize(rep_cls_orig, dim=-1)\n",
    "\n",
    "\n",
    "            cur_sim_matrix_orig = torch.einsum('QE,BE->QB', queries_orig, reps_orig)\n",
    "            full_sim_matrix_orig = torch.cat((full_sim_matrix_orig, cur_sim_matrix_orig.to(full_sim_matrix.device)), 1)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "#             if ind < 4:\n",
    "#                 print(queries[0][:5]) \n",
    "#                 print(reps[0][:5])\n",
    "#                 imshow(query_ims[0])\n",
    "#                 imshow(samples[0])\n",
    "\n",
    "\n",
    "            \n",
    "            ind += 1\n",
    "            print(ind * 100 * args.batch_size / 50000)\n",
    "#             if ind >5:\n",
    "#                 break\n",
    "\n",
    "    vals, inds = torch.topk(full_sim_matrix, num_neighbors)\n",
    "    neighbor_ims = torch.stack([data_loader.dataset[i][0] for i in inds.view(-1)])\n",
    "    \n",
    "    if model_orig is not None:\n",
    "        vals_orig, inds_orig = torch.topk(full_sim_matrix_orig, num_neighbors)\n",
    "        neighbor_ims_orig = torch.stack([data_loader.dataset[i][0] for i in inds_orig.view(-1)])\n",
    "    else:\n",
    "        neighbor_ims_orig = None\n",
    "\n",
    "    return query_ims, neighbor_ims, neighbor_ims_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d7cdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_ims, neighbor_ims, neighbor_ims_orig = nn_evaluate(data_loader=data_loader_val, \n",
    "                                                         model=model,\n",
    "                                                         model_orig=model_orig,\n",
    "                                                         device=DEVICE, \n",
    "                                                         num_queries=50, \n",
    "                                                         num_neighbors=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ac3912",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_neighbors = 20\n",
    "num_neighbors_row = 5\n",
    "\n",
    "for q, im in enumerate(query_ims):\n",
    "    imshow(im)\n",
    "    print('my')\n",
    "    for row in range(num_neighbors // num_neighbors_row):\n",
    "        imshow(rearrange(neighbor_ims[q * num_neighbors + row * num_neighbors_row: q * num_neighbors + (row + 1) * num_neighbors_row], 'B C H W -> C H (B W)'))\n",
    "    print('orig')\n",
    "    for row in range(num_neighbors // num_neighbors_row):\n",
    "        imshow(rearrange(neighbor_ims_orig[q * num_neighbors + row * num_neighbors_row: q * num_neighbors + (row + 1) * num_neighbors_row], 'B C H W -> C H (B W)'))\n",
    "    print('****************************************************************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54196abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_im = query_ims[7].to(DEVICE, non_blocking=True)\n",
    "ref_im = neighbor_ims[141].to(DEVICE, non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83375db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patch_sim_mat(query_im, ref_im, device=DEVICE):\n",
    "    query_im = query_im.to(device, non_blocking=True)\n",
    "    ref_im = ref_im.to(device, non_blocking=True)\n",
    "    \n",
    "    qr_ims = torch.stack([query_im, ref_im]) if query_im.ndim == 3 else torch.cat([query_im, ref_im])\n",
    "    \n",
    "    with torch.cuda.amp.autocast():\n",
    "        qr = model(qr_ims, num_groups=1, group_sz=196, encode_only=True, group_duplicates=False)\n",
    "        qr = qr[:, 1:, :]\n",
    "        q, r = F.normalize(qr)\n",
    "\n",
    "    sim_mat = (q @ r.transpose(-2, -1) / 0.1).softmax(dim=-1)\n",
    "    \n",
    "    return sim_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c107d9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_heat_map(q_im, r_im, sim_mat, q_patch_nums):\n",
    "    qmap_small = torch.zeros(196, device=q_im.device)\n",
    "    qmap_small[q_patch_nums] = 1.\n",
    "    qmap_small = qmap_small.reshape(1,1,14,14).expand(1,3,14,14)\n",
    "    qmap = F.interpolate(qmap_small, size=(224, 224))\n",
    "    \n",
    "    heatmap_small = sim_mat[q_patch_nums].mean(0) * 100\n",
    "    \n",
    "    heatmap_small = heatmap_small.reshape(196).softmax(-1).reshape(14,14)\n",
    "    print(heatmap_small)\n",
    "    heatmap_small = (heatmap_small - heatmap_small.min()) / heatmap_small.max()\n",
    "    heatmap_small = (heatmap_small == heatmap_small.max()).float()\n",
    "    print(heatmap_small)\n",
    "    heatmap_small = torch.stack([heatmap_small]*3)[None, ...]\n",
    "    heatmap = F.interpolate(heatmap_small, size=(224, 224)).squeeze(0)\n",
    "    imshow(qmap * q_im, None)\n",
    "    imshow(r_im, heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82eaa56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_mat = get_patch_sim_mat(query_im, ref_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002eaa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_heat_map(query_im, ref_im, sim_mat, [125])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d438e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(query_ims[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf70131",
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(neighbor_ims[141])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
